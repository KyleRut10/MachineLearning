import pandas as pd
import random as rand
import numpy as np


class MLP:
   # Class Variables
   # num_hidden: Number hidden layers
   # hidden_nodes: List of how many hidden nodes there are per layer
   # num_outputs: Number output nodes
   # train: df of training data
   # test: df of testing data

    def __init__(self, hidden_nodes, num_outputs, training, test, mode):
        self.num_hidden = len(hidden_nodes)
        self.hidden_nodes = hidden_nodes
        self.num_outputs = num_outputs
        self.training = training
        self.test = test
        self.mode = mode
        # Make a cummulative list of how many nodes in each layer
        self.all_layers = [len(training.columns)]
        self.all_layers.extend(hidden_nodes)
        self.all_layers.append(num_outputs)
        self.layers = []
        self.layers.extend(hidden_nodes)
        self.layers.append(num_outputs)
        

    def train(self):
        # Build weight matrices...
        self.weights = []
        
        # W[l]: (n[l], n[l-1]) (h, w)
        for i in range(1, len(self.all_layers)):
            h = self.all_layers[i]
            w = self.all_layers[i-1]
            hw = []
            for val in range(h*w):
                hw.append(rand.uniform(-0.01,0.01))
            # Reshape into 2D Numpy array
            self.weights.append(np.array(hw).reshape(h, w))

            print(self.weights[-1])


        # TODO: Figure out how this is going to work
        converge = False
        while not converge:
            for i,pt in self.training.iterrows():
                #print(pt)
                pass

            # NOTE: Keep weight updates in local variable, then put it in
            # self variable when do final updates

            break
    def forward(self):
        pass

    def backward():
        pass

    def run():
        # Run one example through trained network
        pass
