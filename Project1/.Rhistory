# Inputs -
#     df: The dataframe for the dataset
#     column: the column name that is desired to bin
#     bin_points: A list of the bin points, min is assumed to be less than the
#                 first point and max is added on as infinity
# Output - df: original dataframe but with column replaced with bin values
# add infinity onto end of bin_points so it'll go until the max
bin_points.append(math.inf)
binned = []
for data in df[column]:
for i,bin_point in enumerate(bin_points):
# check if less than point
# don't need to check between, becuase if less than it'd have been
# caught earlier
if data < bin_point:
binned.append('bin{}'.format(i+1))
# break out of loop, because will also be less than others
#print(data, ' ', bin_point, ' ', binned[-1])
break
#print(binned)
df[column] = binned
return df
def scramble_features(df):
# Shuffle values in ~10% of the attributes in the df
#
# Inputs -
#     df: The dataframe for the dataset
# Output - df: original dataframe but with 10% of attributes shuffled
# select how many attributes going to scramble
# subtract 2 from len becuase class is also a column
# add one at end becuase flooring and results could be 0
num_atr_to_shuffle = math.floor(.1*(len(df.columns.values)-2))+1
# randomly select those attributes
atrs_to_shuffle = random.sample(set(df.columns.values[1:]),
num_atr_to_shuffle)
print('reshuffling attribute(s): {}'.format(atrs_to_shuffle))
# reshuffle values in the columns
for atr in atrs_to_shuffle:
df[atr] = np.random.permutation(df[atr].values)
# return the dataframe with shuffled columns
return df
# The datasets have been modified to have the class column first followed
# only by the attribute columns. (Index columns have been removed)
### Respository Datasets
## Breast Cancer
print('*** Brest Cancer Dataset ***')
breastdatastr = os.path.join('Data', 'breast-cancer-wisconsin.csv')
df = pd.read_csv(breastdatastr)
# drop index column
df = df.drop(columns=['index'])
# put class first
df = df.reindex(columns=['class', '1', '2', '3', '4', '5', '6',
'7', '8', '9'])
# put columns back to integers
df.columns = range(df.shape[1])
FirstAlgorithm(df, 1)
# reshuffle
df = scramble_features(df)
FirstAlgorithm(df, 1)
import numpy as np
import pandas as pd
import os
import math
import random
def FirstAlgorithm(rawdata, clean = 0):
# A function to implement a basic machine learning algorithm
#
# Inputs
#   rawdata: A pandas dataframe where the first column
#            contains the class and the following ones
#            contain the attributes
#   clean: A value specifying which of the cleaning
#          functions should be used. Defaults to
#          removing rows with na values
# Data Cleaning Station
if clean == 1:
# Leave the ? as it's own answer (a for simplicity)
# This method is also used if know unkown values
# exist for the dataset in question
data = rawdata.replace(["?"],'a')
else:
# Remove rows with ?s
raw = rawdata.replace(["?"],np.nan)
data = raw.dropna(axis=0,how='any')
# Training and Testing Area
# Values for splitting the data
len_data = len(data)
split = round(len_data/2)
classes = data[0].unique().tolist()
results = np.zeros(10)
# Loops 5 times for performing 5 x 2 Cross Validation
for i in range(0,4):
shuffle = data.sample(frac = 1)
left = shuffle[:split]
right = shuffle[split:]
# Training on the Left and Testing on the Right
results[2*i] = test(right, train(left, classes))
# Training on the Right and Testing on the Left
results[2*i + 1] = test(left, train(right, classes), classes)
def train(data, classes):
# The training function for this algorithm
#
# Inputs -
#   data: The training data for this algorithm
#   classes: The classes found in the original dataset
# Output - storage: An Array holding the Q_C array
#          and the F values for each attribute
storage = {}
Q_C = {}
for i in range(0, len(classes)):
Q_C[i] = sum(data[0] == classes[i])/len(data)
storage[0] = Q_C
for k in range(1, data.shape[1] - 1):
levels = data[k].unique.tolist()
matrix = np.zeros((len(classes),len(levels)))
for i in range(0, len(classes)):
nc = sum(data[0] = classes[i])
for j in range(0, len(levels)):
matrix[i][j] = (sum(data[0] = classes[i] && data[k] = levels[j])\
+ 1)/(nc + data.shape[1] - 2)
storage[2*k - 1] = levels
storage[2*k] = matrix
return(storage)
def test(data, params, classes):
# The testing function for this algorithm
#
# Inputs -
#   data: The testing data for this algorithm
#   params: The parameters calculated in training the algorithm
#   which are used to classify the test set of 'data'
# Output - confusion: The confusion matrix of the classification
# Initialize our confusion Matrix
confusion = np.zeros((len(classes), len(classes)))
# Loop through the rows of our testing data
for r in range(0, len(data)):
# Initialize prediction value array
Cx = np.ones(len(classes))
# Populate with Classification Step
for i in range(0, len(classes)):
Cx[i] = Cx[i] * params[0][i]
# Loop through Attributes
for k in range(1, data.shape[1] - 1):
lvl = storage[2*k - 1].index(data[k][r])
Cx[i] = Cx[i] * storage[2*k][i][lvl]
# Find Positions of the Prediction and Ground Truth
pred = np.argmax(Cx)
true = classes.index(data[0][r])
# Increment the correct element of the confusion matrix
confusion[pred][true] = confusion[pred][true] + 1
return confusion
def bin_column(df, column, bin_points):
# Puts values in a column into bins acording to bin_points
#
# Inputs -
#     df: The dataframe for the dataset
#     column: the column name that is desired to bin
#     bin_points: A list of the bin points, min is assumed to be less than the
#                 first point and max is added on as infinity
# Output - df: original dataframe but with column replaced with bin values
# add infinity onto end of bin_points so it'll go until the max
bin_points.append(math.inf)
binned = []
for data in df[column]:
for i,bin_point in enumerate(bin_points):
# check if less than point
# don't need to check between, becuase if less than it'd have been
# caught earlier
if data < bin_point:
binned.append('bin{}'.format(i+1))
# break out of loop, because will also be less than others
#print(data, ' ', bin_point, ' ', binned[-1])
break
#print(binned)
df[column] = binned
return df
def scramble_features(df):
# Shuffle values in ~10% of the attributes in the df
#
# Inputs -
#     df: The dataframe for the dataset
# Output - df: original dataframe but with 10% of attributes shuffled
# select how many attributes going to scramble
# subtract 2 from len becuase class is also a column
# add one at end becuase flooring and results could be 0
num_atr_to_shuffle = math.floor(.1*(len(df.columns.values)-2))+1
# randomly select those attributes
atrs_to_shuffle = random.sample(set(df.columns.values[1:]),
num_atr_to_shuffle)
print('reshuffling attribute(s): {}'.format(atrs_to_shuffle))
# reshuffle values in the columns
for atr in atrs_to_shuffle:
df[atr] = np.random.permutation(df[atr].values)
# return the dataframe with shuffled columns
return df
# The datasets have been modified to have the class column first followed
# only by the attribute columns. (Index columns have been removed)
### Respository Datasets
## Breast Cancer
print('*** Brest Cancer Dataset ***')
breastdatastr = os.path.join('Data', 'breast-cancer-wisconsin.csv')
df = pd.read_csv(breastdatastr)
# drop index column
df = df.drop(columns=['index'])
# put class first
df = df.reindex(columns=['class', '1', '2', '3', '4', '5', '6',
'7', '8', '9'])
# put columns back to integers
df.columns = range(df.shape[1])
FirstAlgorithm(df, 1)
# reshuffle
df = scramble_features(df)
FirstAlgorithm(df, 1)
import numpy as np
import pandas as pd
import os
import math
import random
def FirstAlgorithm(rawdata, clean = 0):
# A function to implement a basic machine learning algorithm
#
# Inputs
#   rawdata: A pandas dataframe where the first column
#            contains the class and the following ones
#            contain the attributes
#   clean: A value specifying which of the cleaning
#          functions should be used. Defaults to
#          removing rows with na values
# Data Cleaning Station
if clean == 1:
# Leave the ? as it's own answer (a for simplicity)
# This method is also used if know unkown values
# exist for the dataset in question
data = rawdata.replace(["?"],'a')
else:
# Remove rows with ?s
raw = rawdata.replace(["?"],np.nan)
data = raw.dropna(axis=0,how='any')
# Training and Testing Area
# Values for splitting the data
len_data = len(data)
split = round(len_data/2)
classes = data[0].unique().tolist()
results = np.zeros(10)
# Loops 5 times for performing 5 x 2 Cross Validation
for i in range(0,4):
shuffle = data.sample(frac = 1)
left = shuffle[:split]
right = shuffle[split:]
# Training on the Left and Testing on the Right
results[2*i] = test(right, train(left, classes))
# Training on the Right and Testing on the Left
results[2*i + 1] = test(left, train(right, classes), classes)
def train(data, classes):
# The training function for this algorithm
#
# Inputs -
#   data: The training data for this algorithm
#   classes: The classes found in the original dataset
# Output - storage: An Array holding the Q_C array
#          and the F values for each attribute
storage = {}
Q_C = {}
for i in range(0, len(classes)):
Q_C[i] = sum(data[0] == classes[i])/len(data)
storage[0] = Q_C
for k in range(1, data.shape[1] - 1):
levels = data[k].unique.tolist()
matrix = np.zeros((len(classes),len(levels)))
for i in range(0, len(classes)):
nc = sum(data[0] = classes[i])
for j in range(0, len(levels)):
matrix[i][j] = (sum(data[0] = classes[i] && data[k] = levels[j])\
+ 1)/(nc + data.shape[1] - 2)
storage[2*k - 1] = levels
storage[2*k] = matrix
return(storage)
def test(data, params, classes):
# The testing function for this algorithm
#
# Inputs -
#   data: The testing data for this algorithm
#   params: The parameters calculated in training the algorithm
#   which are used to classify the test set of 'data'
# Output - confusion: The confusion matrix of the classification
# Initialize our confusion Matrix
confusion = np.zeros((len(classes), len(classes)))
# Loop through the rows of our testing data
for r in range(0, len(data)):
# Initialize prediction value array
Cx = np.ones(len(classes))
# Populate with Classification Step
for i in range(0, len(classes)):
Cx[i] = Cx[i] * params[0][i]
# Loop through Attributes
for k in range(1, data.shape[1] - 1):
lvl = storage[2*k - 1].index(data[k][r])
Cx[i] = Cx[i] * storage[2*k][i][lvl]
# Find Positions of the Prediction and Ground Truth
pred = np.argmax(Cx)
true = classes.index(data[0][r])
# Increment the correct element of the confusion matrix
confusion[pred][true] = confusion[pred][true] + 1
return confusion
def bin_column(df, column, bin_points):
# Puts values in a column into bins acording to bin_points
#
# Inputs -
#     df: The dataframe for the dataset
#     column: the column name that is desired to bin
#     bin_points: A list of the bin points, min is assumed to be less than the
#                 first point and max is added on as infinity
# Output - df: original dataframe but with column replaced with bin values
# add infinity onto end of bin_points so it'll go until the max
bin_points.append(math.inf)
binned = []
for data in df[column]:
for i,bin_point in enumerate(bin_points):
# check if less than point
# don't need to check between, becuase if less than it'd have been
# caught earlier
if data < bin_point:
binned.append('bin{}'.format(i+1))
# break out of loop, because will also be less than others
#print(data, ' ', bin_point, ' ', binned[-1])
break
#print(binned)
df[column] = binned
return df
def scramble_features(df):
# Shuffle values in ~10% of the attributes in the df
#
# Inputs -
#     df: The dataframe for the dataset
# Output - df: original dataframe but with 10% of attributes shuffled
# select how many attributes going to scramble
# subtract 2 from len becuase class is also a column
# add one at end becuase flooring and results could be 0
num_atr_to_shuffle = math.floor(.1*(len(df.columns.values)-2))+1
# randomly select those attributes
atrs_to_shuffle = random.sample(set(df.columns.values[1:]),
num_atr_to_shuffle)
print('reshuffling attribute(s): {}'.format(atrs_to_shuffle))
# reshuffle values in the columns
for atr in atrs_to_shuffle:
df[atr] = np.random.permutation(df[atr].values)
# return the dataframe with shuffled columns
return df
# The datasets have been modified to have the class column first followed
# only by the attribute columns. (Index columns have been removed)
### Respository Datasets
## Breast Cancer
print('*** Brest Cancer Dataset ***')
breastdatastr = os.path.join('Data', 'breast-cancer-wisconsin.csv')
df = pd.read_csv(breastdatastr)
# drop index column
df = df.drop(columns=['index'])
# put class first
df = df.reindex(columns=['class', '1', '2', '3', '4', '5', '6',
'7', '8', '9'])
# put columns back to integers
df.columns = range(df.shape[1])
FirstAlgorithm(df, 1)
import numpy as np
import pandas as pd
import os
import math
import random
def FirstAlgorithm(rawdata, clean = 0):
# A function to implement a basic machine learning algorithm
#
# Inputs
#   rawdata: A pandas dataframe where the first column
#            contains the class and the following ones
#            contain the attributes
#   clean: A value specifying which of the cleaning
#          functions should be used. Defaults to
#          removing rows with na values
# Data Cleaning Station
if clean == 1:
# Leave the ? as it's own answer (a for simplicity)
# This method is also used if know unkown values
# exist for the dataset in question
data = rawdata.replace(["?"],'a')
else:
# Remove rows with ?s
raw = rawdata.replace(["?"],np.nan)
data = raw.dropna(axis=0,how='any')
# Training and Testing Area
# Values for splitting the data
len_data = len(data)
split = round(len_data/2)
classes = data[0].unique().tolist()
results = np.zeros(10)
# Loops 5 times for performing 5 x 2 Cross Validation
for i in range(0,4):
shuffle = data.sample(frac = 1)
left = shuffle[:split]
right = shuffle[split:]
# Training on the Left and Testing on the Right
results[2*i] = test(right, train(left, classes))
# Training on the Right and Testing on the Left
results[2*i + 1] = test(left, train(right, classes), classes)
def train(data, classes):
# The training function for this algorithm
#
# Inputs -
#   data: The training data for this algorithm
#   classes: The classes found in the original dataset
# Output - storage: An Array holding the Q_C array
#          and the F values for each attribute
storage = {}
Q_C = {}
for i in range(0, len(classes)):
Q_C[i] = sum(data[0] == classes[i])/len(data)
storage[0] = Q_C
for k in range(1, data.shape[1] - 1):
levels = data[k].unique.tolist()
matrix = np.zeros((len(classes),len(levels)))
for i in range(0, len(classes)):
nc = sum(data[0] = classes[i])
for j in range(0, len(levels)):
matrix[i][j] = (sum(data[0] = classes[i] && data[k] = levels[j])\
+ 1)/(nc + data.shape[1] - 2)
storage[2*k - 1] = levels
storage[2*k] = matrix
return(storage)
def test(data, params, classes):
# The testing function for this algorithm
#
# Inputs -
#   data: The testing data for this algorithm
#   params: The parameters calculated in training the algorithm
#   which are used to classify the test set of 'data'
# Output - confusion: The confusion matrix of the classification
# Initialize our confusion Matrix
confusion = np.zeros((len(classes), len(classes)))
# Loop through the rows of our testing data
for r in range(0, len(data)):
# Initialize prediction value array
Cx = np.ones(len(classes))
# Populate with Classification Step
for i in range(0, len(classes)):
Cx[i] = Cx[i] * params[0][i]
# Loop through Attributes
for k in range(1, data.shape[1] - 1):
lvl = storage[2*k - 1].index(data[k][r])
Cx[i] = Cx[i] * storage[2*k][i][lvl]
# Find Positions of the Prediction and Ground Truth
pred = np.argmax(Cx)
true = classes.index(data[0][r])
# Increment the correct element of the confusion matrix
confusion[pred][true] = confusion[pred][true] + 1
return confusion
def bin_column(df, column, bin_points):
# Puts values in a column into bins acording to bin_points
#
# Inputs -
#     df: The dataframe for the dataset
#     column: the column name that is desired to bin
#     bin_points: A list of the bin points, min is assumed to be less than the
#                 first point and max is added on as infinity
# Output - df: original dataframe but with column replaced with bin values
# add infinity onto end of bin_points so it'll go until the max
bin_points.append(math.inf)
binned = []
for data in df[column]:
for i,bin_point in enumerate(bin_points):
# check if less than point
# don't need to check between, becuase if less than it'd have been
# caught earlier
if data < bin_point:
binned.append('bin{}'.format(i+1))
# break out of loop, because will also be less than others
#print(data, ' ', bin_point, ' ', binned[-1])
break
#print(binned)
df[column] = binned
return df
def scramble_features(df):
# Shuffle values in ~10% of the attributes in the df
#
# Inputs -
#     df: The dataframe for the dataset
# Output - df: original dataframe but with 10% of attributes shuffled
# select how many attributes going to scramble
# subtract 2 from len becuase class is also a column
# add one at end becuase flooring and results could be 0
num_atr_to_shuffle = math.floor(.1*(len(df.columns.values)-2))+1
# randomly select those attributes
atrs_to_shuffle = random.sample(set(df.columns.values[1:]),
num_atr_to_shuffle)
print('reshuffling attribute(s): {}'.format(atrs_to_shuffle))
# reshuffle values in the columns
for atr in atrs_to_shuffle:
df[atr] = np.random.permutation(df[atr].values)
# return the dataframe with shuffled columns
return df
# The datasets have been modified to have the class column first followed
# only by the attribute columns. (Index columns have been removed)
### Respository Datasets
## Breast Cancer
print('*** Brest Cancer Dataset ***')
breastdatastr = os.path.join('Data', 'breast-cancer-wisconsin.csv')
df = pd.read_csv(breastdatastr)
# drop index column
df = df.drop(columns=['index'])
# put class first
df = df.reindex(columns=['class', '1', '2', '3', '4', '5', '6',
'7', '8', '9'])
# put columns back to integers
df.columns = range(df.shape[1])
FirstAlgorithm(df, 1)
